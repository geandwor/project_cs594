{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from collections import defaultdict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do data preprocessings\n",
    "Do visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''stats on the metadata\n",
    "   data is signature based \n",
    "   dict to record the properties for each signature\n",
    "'''\n",
    "metasigset = set()\n",
    "reasondict = {}\n",
    "proxexdict = {}\n",
    "proxintdict = {}\n",
    "normproxexdict ={}\n",
    "normproxintdict ={}\n",
    "numexdict = {}\n",
    "numintdict = {}\n",
    "signamedict = {}\n",
    "signaturedict = {}\n",
    "describdict={}\n",
    "describset = set()\n",
    "sigtypenumberdict = {}\n",
    "sigtypenamedict = {}\n",
    "sigtypenumberstat= {}\n",
    "sigtypenamestat = {}\n",
    "sigtypenumberset = set()\n",
    "sigtypesignamedict = {}\n",
    "sigtypesigname = {}\n",
    "'''group signatures according to their sigtypename or number'''\n",
    "sigtypesignumberdic = defaultdict(list)\n",
    "sigtypenumsignumberdic = defaultdict(list)\n",
    "\n",
    "'''record the normalized scores to two list for histogram'''\n",
    "normInthisto = []\n",
    "normExthisto = []\n",
    "\n",
    "'''record the normalized score to two dict for each signumber'''\n",
    "normIntsignumber ={}\n",
    "normExtsignumber ={}\n",
    "\n",
    "'''sigtypenumber and normalized proximity dict of list\n",
    "'''\n",
    "\n",
    "sigtypenumberExt = defaultdict(list)\n",
    "sigtypenumberInt = defaultdict(list)\n",
    "\n",
    "'''lambda function to calculate the normalized proximity score'''\n",
    "normInt = lambda x: float(x[2])/float(x[4]) if x[2]!=\"\" and x[2]!='None' and x[4]!=\"\" and x[4]!='None'  and float(x[4])>0 else 0\n",
    "normExt = lambda x: float(x[3])/float(x[5]) if x[3]!=\"\" and x[3]!='None'and x[5]!=\"\" and x[5]!='None'  and float(x[5])>0 else 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "#do some initial analysis, split each line and get the data into the correaponding dict\n",
    "fname1 = \"/Users/grandwor/Desktop/research_stat/sigmetadata.txt\"\n",
    "f1 = open(fname1, 'r')\n",
    "f1csvread = csv.reader(f1, delimiter = '\\t')\n",
    "rowcount=0\n",
    "for row in f1csvread:\n",
    "    print(\"line is \", rowcount)\n",
    "    if rowcount ==0:\n",
    "        rowcount+=1\n",
    "        continue;\n",
    "    else:\n",
    "        if(row[3]!=\"\" and row[3]!='None'):\n",
    "            print(\"row3 raw\",float(row[3]))\n",
    "        metasigset.add(row[0])\n",
    "        rowcount +=1\n",
    "        reasondict[row[0]] = row[1]\n",
    "        proxexdict[row[0]] =row[2]\n",
    "        proxintdict[row[0]] =row[3]\n",
    "        numexdict[row[0]] =row[4]\n",
    "        numintdict[row[0]] = row[5]\n",
    "        '''normalize external and internal proximity'''\n",
    "        if(row[1].isdigit() and float(row[1])>0):\n",
    "            normproxexdict[row[0]] = 1\n",
    "        else:\n",
    "            if(row[2]!=\"\" and row[2]!='None' and row[4]!=\"\" and row[4]!='None' and float(row[4])>0):\n",
    "                \n",
    "                normproxexdict[row[0]] = float(row[2])/float(row[4])\n",
    "            else:\n",
    "                normproxexdict[row[0]] = 0\n",
    "        normExthisto.append(normproxexdict[row[0]])\n",
    "        sigtypenumberExt[row[9]].append(normproxexdict[row[0]])\n",
    "        normExtsignumber[row[0]] = normproxexdict[row[0]]\n",
    "        \n",
    "        if(row[1].isdigit() and float(row[1])>0):\n",
    "            normproxintdict[row[0]] = 1\n",
    "        else:\n",
    "            if(row[3]!=\"\" and row[3]!='None'and row[5]!=\"\" and row[5]!='None' and float(row[5])>0):\n",
    "                \n",
    "                normproxintdict[row[0]] = float(row[3])/float(row[5])\n",
    "            else:            \n",
    "                normproxintdict[row[0]] =0\n",
    "        normInthisto.append(normproxintdict[row[0]])\n",
    "        sigtypenumberInt[row[9]].append(normproxintdict[row[0]])\n",
    "        normIntsignumber[row[0]] = normproxintdict[row[0]]\n",
    "        \n",
    "        '''print(\"normext\", normproxexdict[row[0]])\n",
    "        print(\"normint\", normproxintdict[row[0]])'''\n",
    "        \n",
    "        signamedict[row[0]] = row[6]\n",
    "        signaturedict[row[0]] = row[7]\n",
    "        describdict[row[0]] = row[8]\n",
    "        describset.add(row[8])\n",
    "        sigtypenumberdict[row[0]]=row[9]\n",
    "        sigtypenumberset.add(row[9])\n",
    "        sigtypenamedict[row[0]] =row[10]\n",
    "        \n",
    "        if not(sigtypesigname.has_key(row[9])):\n",
    "            sigtypesigname[row[9]] = row[10]\n",
    "        \n",
    "        sigtypesignumberdic[row[10]].append(row[0])\n",
    "        sigtypenumsignumberdic[row[9]].append(row[0])\n",
    "        \n",
    "        if sigtypenumberstat.has_key(row[9]):\n",
    "            sigtypenumberstat[row[9]]+=1\n",
    "        else:\n",
    "            sigtypenumberstat[row[9]]=1\n",
    "        if sigtypenamestat.has_key(row[10]):\n",
    "            sigtypenamestat[row[10]] +=1\n",
    "        else:\n",
    "            sigtypenamestat[row[10]] =1\n",
    "f1.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization of initial data analysis\n",
    "plt.hist(normInthisto)\n",
    "plt.savefig(\"/home/zwen/Desktop/research_stat/\"+\"normInthisto.png\")\n",
    "plt.hist(normExthisto)\n",
    "plt.savefig(\"/home/zwen/Desktop/research_stat/\"+\"normExthisto.png\")\n",
    "len(sigtypenumberset)\n",
    "for sig in sigtypenumberset:\n",
    "    plt.hist(sigtypenumberInt[sig])\n",
    "    plt.hist(sigtypenumberExt[sig])\n",
    "plt.bar(list(sigtypenumberstat.keys()), sigtypenumberstat.values(), color='g')\n",
    "\n",
    "lstatforsigpersigtypenumber=[]        \n",
    "for st in sigtypenumberset:\n",
    "    lstatforsigpersigtypenumber.append(len(sigtypenumsignumberdic[st]))\n",
    "\n",
    "plt.hist(lstatforsigpersigtypenumber)\n",
    "plt.xlabel('signature count per sigtype')\n",
    "plt.ylabel('frq count')\n",
    "plt.title(\"Histogram of Signatures per Sigtype\")\n",
    "plt.savefig(\"/home/zwen/Desktop/stat/\"+\"lstatforsigpersigtypenumber.pdf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''stat on the data'''\n",
    "\n",
    "fname = \"/Users/grandwor/Desktop/research_stat/collapsedSpertus201607to10.txt\"\n",
    "f = open(fname,'r')\n",
    "'''\n",
    "sets for the machineid, IPaddress inccategory attached_incidenttype signature, columnname\n",
    "dict signature-frequency total data\n",
    "dict signature -fequency for each log\n",
    "\n",
    "\n",
    "'''\n",
    "mcIDset = set()\n",
    "colname = set()\n",
    "ipset = set()\n",
    "incTypelib ={}\n",
    "incCatlib ={}\n",
    "sigset = set()\n",
    "sigfreqTotal = {}\n",
    "siglog = {}\n",
    "sigfreqlog = {}\n",
    "sigfreqlogIntscore ={}\n",
    "sigfreqlogExtscore ={}\n",
    "sigfreqlogIntscoretimesfreq = {}\n",
    "sigfreqlogExtscoretimesfreq = {}\n",
    "isinc = {}\n",
    "'''siglist to record all the occurrences of signatures for each type'''\n",
    "siglist ={}\n",
    "isincstat ={'t':0, 'f':0}\n",
    "'''machineid-signature type or signature type name'''\n",
    "mIDsigtypename = defaultdict(set)\n",
    "mIDsigtypenumber=defaultdict(set)\n",
    "'''\n",
    "signatures count across all the data\n",
    "'''\n",
    "sigdatacount = {}\n",
    "\n",
    "'''sigtypenumber-signaturecount'''\n",
    "sigtypenumbersignaturecount = {}\n",
    "\n",
    "'''logname with label true and false'''\n",
    "lognamet = set()\n",
    "lognamef = set()\n",
    "\n",
    "'''arountinc'''\n",
    "aroundinc = {}\n",
    "\n",
    "'''csv reader'''\n",
    "fcsvread = csv.reader(f, delimiter='\\t')\n",
    "lognameset =set()\n",
    "'''row count'''\n",
    "\n",
    "'''number of logs'''\n",
    "'''machineID for date'''\n",
    "mcIDperdate = defaultdict(list)\n",
    "'''machineID for events'''\n",
    "mcIDperevents = defaultdict(list)\n",
    "'''logpermachine'''\n",
    "numbereventsperlog = []\n",
    "'''clusdate'''\n",
    "clusdateset = set()\n",
    "\n",
    "'''signature logdata'''\n",
    "logsignature =defaultdict(list)\n",
    "\n",
    "count = 0\n",
    "for row in fcsvread:\n",
    "    print(\"rowcount is \",count)\n",
    "    if (count==0 or count==224720 or count==224721 or count==258491 or count==258492 or count==294542) :\n",
    "        rowlen = len(row)\n",
    "        print(row)\n",
    "        for i in range(rowlen):\n",
    "            colname.add(row[i])\n",
    "        count +=1\n",
    "        continue;\n",
    "    else:\n",
    "        count+=1\n",
    "        mcIDset.add(row[0])\n",
    "        \n",
    "        if  incCatlib.has_key(row[3]):\n",
    "            incCatlib[row[3]]+=1\n",
    "        else:\n",
    "            incCatlib[row[3]] =1\n",
    "        \n",
    "        if  incTypelib.has_key(row[4]):\n",
    "            incTypelib[row[4]]+=1\n",
    "        else:\n",
    "            incTypelib[row[4]] =1\n",
    "        \n",
    "        '''generate a dict for each log to record the sig, fre pair'''\n",
    "        '''name for log dict is the machineid and incdate'''\n",
    "        '''incident for logdata'''\n",
    "        clusdate = row[1].split(\" \")\n",
    "        incdate = row[2].split(\" \")\n",
    "        logname = row[0]+clusdate[0]\n",
    "        numbereventsperlog.append(int(row[6]))\n",
    "        lognameset.add(logname)\n",
    "        mcIDperdate[clusdate[0]].append(row[0])\n",
    "        clusdateset.add(clusdate[0])\n",
    "        aroundinc[logname] = row[5]\n",
    "        if(row[5]=='t' and clusdate[0]==incdate[0]):\n",
    "            isinc[logname] = 't'\n",
    "            isincstat['t'] +=1\n",
    "            lognamet.add(logname)\n",
    "        else:\n",
    "            isinc[logname] = 'f'\n",
    "            isincstat['f']+=1\n",
    "            lognamef.add(logname)\n",
    "        '''dict for each log signatures'''\n",
    "        sigfreqlog[logname] = {}\n",
    "        siglog[logname]=set()\n",
    "        sigfreqlogIntscore[logname] ={}\n",
    "        sigfreqlogExtscore[logname] ={}\n",
    "        sigfreqlogIntscoretimesfreq[logname]={}\n",
    "        sigfreqlogExtscoretimesfreq[logname]={} \n",
    "        '''row[7] has to remove the ()'''\n",
    "        sigfreq = row[7].replace(\"(\", \"\")\n",
    "        sigfreq = sigfreq.replace(\";)\", \"\")\n",
    "        sigfreqset= sigfreq.split(\";\")\n",
    "        print(sigfreqset)\n",
    "        '''put signumber and frequency count to the log library for each log'''\n",
    "        if(len(sigfreqset)>0):\n",
    "            for sf in sigfreqset:\n",
    "                signum= sf.split(\",\")\n",
    "                sigset.add(signum[0])\n",
    "               \n",
    "                if(count<=5):\n",
    "                    print(signum[0])\n",
    "                if(len(signum)>1):\n",
    "                    ''''print(\"row count\", count)'''\n",
    "                     \n",
    "                    sigfreqlog[logname][signum[0]] =signum[1]\n",
    "                    if(signum[0]!='-1'):\n",
    "                        mcIDperevents[signum[0]].append(row[0])\n",
    "                        if sigtypenumbersignaturecount.has_key(sigtypenumberdict[signum[0]]):\n",
    "                            sigtypenumbersignaturecount[sigtypenumberdict[signum[0]]] +=1\n",
    "                        else:\n",
    "                            sigtypenumbersignaturecount[sigtypenumberdict[signum[0]]] =1\n",
    "                        \n",
    "                        if sigdatacount.has_key(signum[0]):\n",
    "                            sigdatacount[signum[0]] +=1\n",
    "                        else:\n",
    "                            sigdatacount[signum[0]] = 1\n",
    "                        mIDsigtypename[row[0]].add(sigtypenamedict[signum[0]])\n",
    "                        mIDsigtypenumber[row[0]].add(sigtypenumberdict[signum[0]])\n",
    "                        if(siglist.has_key(sigtypenumberdict[signum[0]])):\n",
    "                            siglist[sigtypenumberdict[signum[0]]]+=1\n",
    "                        else:\n",
    "                            siglist[sigtypenumberdict[signum[0]]] =1\n",
    "                        \n",
    "                    '''there is a case where signumber is -1 which is not in the metadata\n",
    "                    I assigned a internal and external importance 0.1 for now\n",
    "                    '''\n",
    "                    if(signum[0]=='-1'):\n",
    "                        normIntsignumber[signum[0]] = 0.1\n",
    "                        normExtsignumber[signum[0]] =0.1\n",
    "                    logsignature[signum[0]].append(logname)\n",
    "                    siglog[logname].add(signum[0])\n",
    "                    sigfreqlog[logname][signum[0]] = int(signum[1])   \n",
    "                    sigfreqlogIntscore[logname][signum[0]] =normIntsignumber[signum[0]]\n",
    "                    sigfreqlogExtscore[logname][signum[0]] =normExtsignumber[signum[0]]\n",
    "                    sigfreqlogIntscoretimesfreq[logname][signum[0]] =int(signum[1])*float(normIntsignumber[signum[0]])\n",
    "                    sigfreqlogExtscoretimesfreq[logname][signum[0]] =int(signum[1])*float(normExtsignumber[signum[0]])\n",
    "f.close()\n",
    "\n",
    "'''number of machineid per date and histogram'''\n",
    "lmachineIDfordate=[]\n",
    "for date in clusdateset:\n",
    "    lmachineIDfordate.append(len(mcIDperdate[date]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization\n",
    "'''number of machineid per date and histogram'''\n",
    "lmachineIDfordate=[]\n",
    "for date in clusdateset:\n",
    "    lmachineIDfordate.append(len(mcIDperdate[date]))\n",
    "    \n",
    "plt.hist(lmachineIDfordate)\n",
    "plt.title(\"Histogram for number of machineid for dates\")\n",
    "plt.xlabel(\"number of machineIDs\")\n",
    "plt.ylabel(\"freq count\")\n",
    "plt.savefig(\"/home/zwen/Desktop/stat/\"+\"machineIDfordtate.png\")\n",
    " \n",
    "'''number of machineid per each event''' \n",
    "lmachineIDforevent=[]\n",
    "for sig in sigset:\n",
    "    lmachineIDforevent.append(len(mcIDperevents[sig]))\n",
    "plt.hist(lmachineIDforevent)\n",
    "plt.title(\"Histogram for number of machineid for events\")\n",
    "plt.xlabel(\"number of machineIDs\")\n",
    "plt.ylabel(\"freq count\")\n",
    "plt.savefig(\"/home/zwen/Desktop/stat/\"+\"machineIDforevents.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats on machine ID\n",
    "count of machineID - freq\n",
    "         1 - 21113\n",
    "         2 - 4007\n",
    "         3 - 1517\n",
    "         4 - 1058\n",
    "         5 - 804\n",
    "         6 - 600\n",
    "         7  -439\n",
    "         8 - 360\n",
    "         9  -301\n",
    "         10 -264\n",
    "         \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ln in lognameset:\n",
    "    print(\"logname and data \",ln)\n",
    "    print(sigfreqlogIntscoretimesfreq[ln])\n",
    "    print(sigfreqlogExtscoretimesfreq[ln])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''signature happened only a few times'''\n",
    "lsiggreatertime = []\n",
    "for sig in sigdatacount.keys():\n",
    "    if sigdatacount[sig] >=100:\n",
    "        lsiggreatertime.append(sig)\n",
    "    \n",
    "'''stats on the data about sigtypenumber-signumber pair dict\n",
    "  stats on the sigtypename-signumber pair dict\n",
    "'''  \n",
    "typesignumberstat = {}\n",
    "typeNumsignumberstat={} \n",
    "'''sigset and metasigset only difference in one item -1'''\n",
    "for sig in sigset:\n",
    "    if sig == '-1':\n",
    "        continue;\n",
    "    if typesignumberstat.has_key(sigtypenamedict[sig]):\n",
    "        typesignumberstat[sigtypenamedict[sig]] +=1\n",
    "    else:\n",
    "        typesignumberstat[sigtypenamedict[sig]] =1\n",
    "    \n",
    "    if typeNumsignumberstat.has_key(sigtypenumberdict[sig]):\n",
    "        typeNumsignumberstat[sigtypenumberdict[sig]] +=1\n",
    "    else:\n",
    "        typeNumsignumberstat[sigtypenumberdict[sig]] =1\n",
    "    \n",
    "    \n",
    "'''print the histogram of sigtypenumber and proximity scores'''   \n",
    "    \n",
    "statfile = open(\"/home/zwen/Desktop/statfile.txt\",'w') \n",
    "    \n",
    "import sys\n",
    "'''sys.stdout=statfile'''   \n",
    "    \n",
    "for st in sigtypenumberset:\n",
    "    plt.title(\"sigtypenumer \"+st+\" size \"+str(sigtypenumberstat[st])+\" occurrences \"+str(siglist[st]))\n",
    "    plt.hist(sigtypenumberInt[st])\n",
    "    plt.xlabel(\"Normalized Internal Proximity - Sigtypename \"+sigtypesigname[st])\n",
    "    plt.ylabel(\"Countfrq\")\n",
    "    plt.savefig(\"/home/zwen/Desktop/stat/\"+st+\".pdf\")  \n",
    "    \n",
    "for k in sigtypenumberInt.keys():\n",
    "    sys.stdout.write(\"len is \"+str(len(sigtypenumberInt[k])))\n",
    "    sys.stdout.flush()\n",
    "for st in sigtypenumberset:\n",
    "    plt.title(\"sigtypenumer \"+st+\" size \"+str(sigtypenumberstat[st])+\" len of sigtypenumberInt \"+str(len(sigtypenumberInt[st]))+\" occurrences \"+str(siglist[st]))\n",
    "    plt.hist(sigtypenumberInt[st])\n",
    "    plt.xlabel(\"Normalized Internal Proximity - Sigtypename \"+sigtypesigname[st])\n",
    "    plt.ylabel(\"Countfrq\")\n",
    "    plt.savefig(\"/home/zwen/Desktop/stat/\"+st+\".pdf\")\n",
    "    plt.show()\n",
    "\n",
    "lstatforsigtypenumber = [] \n",
    "lstatforsigtypename = [] \n",
    "for id in mcIDset:\n",
    "    lstatforsigtypename.append(len(mIDsigtypename[id]))\n",
    "    lstatforsigtypenumber.append(len(mIDsigtypenumber[id]))\n",
    "    \n",
    "'''draw histogram'''  \n",
    "plt.hist(lstatforsigtypename)\n",
    "plt.title(\"number of type by machineID\")\n",
    "plt.xlabel(\"number of types\")\n",
    "plt.ylabel(\"freq count\")\n",
    "plt.savefig(\"/home/zwen/Desktop/stat/\"+\"numberoftype_byMachineID.pdf\")\n",
    "plt.show()\n",
    "'''\n",
    "number of Type - frequency count\n",
    "1 - 10402\n",
    "2 - 11490\n",
    "3 - 9925\n",
    "4 - 7847\n",
    "5 - 2941\n",
    "6 - 421\n",
    "7 - 135\n",
    "8 - 50\n",
    "9 - 21\n",
    "10 - 11\n",
    "11 - 1\n",
    "12 - 1\n",
    "\n",
    "'''   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "check the machineID with different type of types of signatures\n",
    "make a data set and check the sparsity?????\n",
    "'''  \n",
    "'''\n",
    "generate a histogram of the sigdatacount\n",
    "'''\n",
    "lstatforsigdatacount = []\n",
    "for sig in sigdatacount.keys():\n",
    "    lstatforsigdatacount.append(sigdatacount[sig])\n",
    "'''draw histogram'''  \n",
    "plt.hist(lstatforsigtypename)\n",
    "plt.title(\"Histogram sigdatacount\")\n",
    "plt.xlabel(\"number of count across data\")\n",
    "plt.ylabel(\"freq count\")\n",
    "plt.savefig(\"/home/zwen/Desktop/stat/\"+\"sigdatacount.pdf\")\n",
    "plt.show()\n",
    "'''\n",
    "number of count - freq count\n",
    "1 - 21113\n",
    "2 - 4007\n",
    "3 - 1571\n",
    "4 - 1058\n",
    "5 - 804\n",
    "6 - 600\n",
    "7 - 439\n",
    "8 - 360\n",
    "9 - 301\n",
    "10 - 264\n",
    "11 - 243\n",
    "12 - 204\n",
    "13 - 184\n",
    "14 - 163\n",
    "15 - 147\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''generate histogram for the sigtype - signature count'''\n",
    "lstatforsigtypenumbersignaturecount = []\n",
    "for sig in sigtypenumbersignaturecount.keys():\n",
    "    lstatforsigtypenumbersignaturecount.append(sigtypenumbersignaturecount[sig])\n",
    "'''draw histogram'''  \n",
    "plt.hist(lstatforsigtypenumbersignaturecount)\n",
    "plt.title(\"Histogram sigtypesignaturecount\")\n",
    "plt.xlabel(\"number of count across signatures\")\n",
    "plt.ylabel(\"freq count\")\n",
    "plt.savefig(\"/home/zwen/Desktop/stat/\"+\"sigtypenumbersignaturecount.pdf\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''generate data set for learning'''  \n",
    "'''\n",
    "1. feature selection,there are 107 categories of signatures, the problem here is \n",
    "to reduce the possible signatures and not contain so much sparsity.\n",
    "\n",
    "   '20': 12942, possible category\n",
    " '2000': 6718, might drop\n",
    " '2002': 4348,might drop\n",
    " '2003': 1170,might drop\n",
    " '2006': 49333,possible category??\n",
    " '2007': 295,might drop?\n",
    " '2008': 229,might frop?\n",
    " '2010': 17, might drop\n",
    " '2012': 180,might drop?\n",
    " '2013': 101,drop\n",
    " '2014': 141,drop\n",
    " '2016': 133455,might drop?\n",
    " '2018': 175,might drop\n",
    " '2019': 4604,possible ?\n",
    " '2023': 1510, possible\n",
    " '2027': 1,drop\n",
    " '2029': 27,drop\n",
    " '2035': 1253,drop?\n",
    " '2038': 28409,drop\n",
    " '2040': 177,drop\n",
    " '2042': 1060,drop?\n",
    " '2046': 117,drop\n",
    " '2048': 349,drop\n",
    " '2049': 357,drop?\n",
    " '21': 40, might drop\n",
    " '28': 88207,possible category\n",
    " '3003': 2012,possible category\n",
    " '3006': 21,might drop\n",
    " '3007': 1843,?\n",
    " '3008': 219,?\n",
    " '3012': 1,drop\n",
    " '3013': 130,drop?\n",
    " '3022': 597,drop\n",
    " '3028': 113,drop\n",
    " '3038': 3831,might drop?\n",
    " '3042': 1,drop\n",
    " '3043': 569,drop\n",
    " '3062': 102,drop\n",
    " '3064': 170,drop\n",
    " '3065': 8,drop\n",
    " '3080': 106,drop\n",
    " '3093': 1, drop\n",
    " '3097': 1736207,possible category\n",
    " '3098': 2587547,possible category\n",
    " '36': 8451, possible category\n",
    " '40': 6082, might drop\n",
    " '41': 21909, possible category\n",
    " '42': 212458, possible category\n",
    " '44': 1369825, possible?\n",
    " '45': 101569,possible?\n",
    " '5037': 96987,possible category\n",
    " '5053': 2393,possible\n",
    " '5058': 10,drop\n",
    " '5063': 862,drop\n",
    " '5064': 27,drop\n",
    " '5072': 13881,possible?\n",
    " '5091': 23483,drop\n",
    " '5095': 1276,drop\n",
    " '5097': 13,drop\n",
    " '5100': 1,drop\n",
    " '5103': 5026,might drop?\n",
    " '5104': 1224,might drop?\n",
    " '5107': 2962,might drop?\n",
    " '5111': 3,drop\n",
    " '5121': 203,drop\n",
    " '5127': 130,drop\n",
    " '5140': 40,drop\n",
    " '5143': 765,drop\n",
    " '5153': 166,drop\n",
    " '5156': 1553,might drop?\n",
    " '5160': 6,drop\n",
    " '5163': 972,drop\n",
    " '5164': 1017,might drop?\n",
    " '5167': 253,drop\n",
    " '5168': 26641,possible category\n",
    " '5171': 3,drop\n",
    " '5172': 7505,drop\n",
    " '5173': 64,drop\n",
    " '5174': 2,drop\n",
    " '5176': 377,drop\n",
    " '5177': 96,drop\n",
    " '5179': 45,drop\n",
    " '5181': 9777,possible category\n",
    " '5182': 1,drop\n",
    " '5185': 23,drop\n",
    " '5186': 4,drop\n",
    " '5191': 17,drop\n",
    " '53': 30192,possible category\n",
    " '55': 60, might drop\n",
    " '5507': 2006,drop\n",
    " '57': 30358, possible category\n",
    " '58': 15702, possible?\n",
    " '59': 22031, less possible?\n",
    " '60': 12287, possible category\n",
    " '63': 3725, possible category\n",
    " '64': 3620, possible category\n",
    " '65': 122, possible?\n",
    " '66': 112, possible?\n",
    " '67': 473, possible?\n",
    " '68': 5,drop\n",
    " '69': 2,drop\n",
    " '70': 294,might drop?\n",
    " '71': 6, might drop?\n",
    " '72': 6449, might drop?\n",
    " '73': 223,might drop?\n",
    " '74': 5, might drop\n",
    " '8': 10174 possible\n",
    " \n",
    " the information above is based on the histogram of each category.\n",
    "\n",
    "'''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keepset = set(['20','2023','28','3003','3097','3098','36','41','42','5037','5053','5168','5181','53','57','60','63','64','8'])\n",
    "possibleset = set(['2006','2019','3007','3008','44','45','5072','58','59''65','66','67'])\n",
    "mightdropset = set(['2000','2002','2003','2007','2008','2010','2012','2016','2018','21','3006','3038','40','5103','5104', '5107','5156','5164','55','70','71','72','73','74'])\n",
    "dropset=set(['2013','2014','2027','2029','2035','2038','2040','2042','2046','2048','2049','3012','3013','3022','3028','3042', '3043', '3062', '3064', '3065', '3080', '3093','5058', '5063', '5064','5091', '5095','5097', '5100','5111', '5121','5127', '5140', '5143', '5153','5160', '5163','5167','5171', '5172', '5173', '5174', '5176', '5177', '5179',  '5182', '5185', '5186', '5191','5507','68','69']) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''then deal with signature in the three sets above differenly'''\n",
    "'''\n",
    "1 first route: treat the signatures in the first into 10 categories based on the normalized score or frequency*normalized score\n",
    "               treat the signature in the mightdropset together, and the signature in the dropset together.  \n",
    "               \n",
    "   for the keepset, generate a eleven variables for this category, range from  0 - 1 by 0.1 with 11 category\n",
    "   k0, k01, k02 - k1\n",
    "   for the possibleset, generate  6 variables, range from 0 - 1 by 0.2\n",
    "   p0,p02-p1\n",
    "   for the mightdropset, generate 3 variables for it range from 0 0.5 and 1\n",
    "   m0 m05 m1\n",
    "   for the drop set generate two variables for 0 and 1\n",
    "   d0 d1\n",
    "   \n",
    "   \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''add field titles'''\n",
    "fieldnames=['logname','aroundinc','label']\n",
    "for i in range(11):\n",
    "    fieldnames.append('k'+str(i))\n",
    "for i in range(0,11,2):\n",
    "    fieldnames.append('p'+str(i))\n",
    "for i in range(0,11,5):\n",
    "    fieldnames.append('m'+str(i))\n",
    "for i in range(0,11,10):\n",
    "    fieldnames.append('d'+str(i))\n",
    "    \n",
    "dataout1 = defaultdict(dict)\n",
    "for lg in lognameset:\n",
    "    \n",
    "    for i in range(11):\n",
    "        dataout1[lg]['k'+str(i)] = 0\n",
    "    for i in range(0,11,2):\n",
    "        dataout1[lg]['p'+str(i)] = 0\n",
    "    for i in range(0,11,5):\n",
    "        dataout1[lg]['m'+str(i)] = 0\n",
    "    for i in range(0,11,10):\n",
    "        dataout1[lg]['d'+str(i)] = 0\n",
    "    dataout1['label'] = 0\n",
    "\n",
    "g = lambda x : 1 if x=='t' else 0    \n",
    "for lg in lognameset:\n",
    "    dataout1[lg]['aroundinc']= g(aroundinc[lg])\n",
    "    dataout1[lg]['logname'] = lg\n",
    "    if isinc[lg] == 't':\n",
    "        dataout1[lg]['label'] = 1\n",
    "    else:\n",
    "        dataout1[lg]['label'] = 0\n",
    "        \n",
    "    for sig in sigfreqlogIntscore[lg].keys():\n",
    "        sk = sigfreqlogIntscore[lg][sig]\n",
    "        if sig!= '-1' and sigtypenumberdict[sig] in keepset:\n",
    "            if sk == 0:\n",
    "                dataout1[lg]['k0'] +=1\n",
    "            if sk>0 and sk<0.1:\n",
    "                dataout1[lg]['k1'] +=1\n",
    "            if sk>=0.1 and sk<0.2:\n",
    "                dataout1[lg]['k2'] +=1\n",
    "            if sk>=0.2 and sk<0.3:\n",
    "                dataout1[lg]['k3'] +=1\n",
    "            if sk>=0.3 and sk<0.4:\n",
    "                dataout1[lg]['k4'] +=1\n",
    "            if sk>=0.4 and sk<0.5:\n",
    "                dataout1[lg]['k5'] +=1\n",
    "            if sk>=0.5 and sk<0.6:\n",
    "                dataout1[lg]['k6'] +=1\n",
    "            if sk>=0.6 and sk<0.7:\n",
    "                dataout1[lg]['k7'] +=1\n",
    "            if sk>=0.7 and sk<0.8:\n",
    "                dataout1[lg]['k8'] +=1\n",
    "            if sk>=0.8 and sk<0.9:\n",
    "                dataout1[lg]['k9'] +=1\n",
    "            if sk>=0.9 and sk<=1:\n",
    "                dataout1[lg]['k10'] +=1\n",
    "                \n",
    "        if sig!= '-1' and sigtypenumberdict[sig] in possibleset:\n",
    "            if sk == 0:\n",
    "                dataout1[lg]['p0'] +=1\n",
    "            if sk>0 and sk<0.2:\n",
    "                dataout1[lg]['p2'] +=1\n",
    "            if sk>=0.2 and sk<0.4:\n",
    "                dataout1[lg]['p4'] +=1\n",
    "            if sk>=0.4 and sk<0.6:\n",
    "                dataout1[lg]['p6'] +=1\n",
    "            if sk>=0.6 and sk<0.8:\n",
    "                dataout1[lg]['p8'] +=1\n",
    "            if sk>=0.8 and sk<=1:\n",
    "                dataout1[lg]['p10'] +=1\n",
    "                \n",
    "        if sig!='-1' and sigtypenumberdict[sig] in mightdropset:\n",
    "            if sk == 0:\n",
    "                dataout1[lg]['m0'] +=1\n",
    "            if sk>0 and sk<0.5:\n",
    "                dataout1[lg]['m5'] +=1\n",
    "            if sk>=0.5 and sk<=1:\n",
    "                dataout1[lg]['m10'] +=1\n",
    "        if sig!='-1' and sigtypenumberdict[sig] in dropset:\n",
    "            if sk < 0.8:\n",
    "                dataout1[lg]['d0'] +=1\n",
    "            if sk>=0.8 and sk<=1:\n",
    "                dataout1[lg]['d10'] +=1\n",
    "#save the data for later usage\n",
    "fopen1 = open(\"/home/zwen/Desktop/stat/m1.csv\",'w')\n",
    "writer = csv.DictWriter(fopen1, fieldnames=fieldnames)\n",
    "writer.writeheader()\n",
    "for lg in lognameset:\n",
    "    writer.writerow(dataout1[lg])\n",
    "fopen1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "second use the frequency*normInt score \n",
    "'''\n",
    "fieldnames1=['logname','aroundinc','label']\n",
    "for i in range(11):\n",
    "    fieldnames1.append('k'+str(i))\n",
    "for i in range(11):\n",
    "    fieldnames1.append('p'+str(i))\n",
    "for i in range(2):\n",
    "    fieldnames1.append('m'+str(i))\n",
    "for i in range(2):\n",
    "    fieldnames1.append('d'+str(i))\n",
    "    \n",
    "dataout2 = defaultdict(dict)\n",
    "for lg in lognameset:\n",
    "    for i in range(11):\n",
    "        dataout2[lg]['k'+str(i)] = 0\n",
    "    for i in range(11):\n",
    "        dataout2[lg]['p'+str(i)] = 0\n",
    "    for i in range(2):\n",
    "        dataout2[lg]['m'+str(i)] = 0\n",
    "    for i in range(2):\n",
    "        dataout2[lg]['d'+str(i)] = 0\n",
    "    dataout2[lg]['label'] = 0\n",
    "    \n",
    "  \n",
    "g = lambda x : 1 if x=='t' else 0    \n",
    "for lg in lognameset:\n",
    "    dataout2[lg]['aroundinc']= g(aroundinc[lg])\n",
    "    dataout2[lg]['logname'] = lg\n",
    "    if isinc[lg] == 't':\n",
    "        dataout2[lg]['label'] = 1\n",
    "    else:\n",
    "        dataout2[lg]['label'] = 0\n",
    "        \n",
    "    for sig in sigfreqlogIntscore[lg].keys():\n",
    "        sk = sigfreqlogIntscoretimesfreq[lg][sig]\n",
    "        if sig!= '-1' and sigtypenumberdict[sig] in keepset:\n",
    "            if sk == 0:\n",
    "                dataout2[lg]['k0'] +=1\n",
    "            if sk>0 and sk<1:\n",
    "                dataout2[lg]['k1'] +=1\n",
    "            if sk>=1 and sk<2:\n",
    "                dataout2[lg]['k2'] +=1\n",
    "            if sk>=2 and sk<3:\n",
    "                dataout2[lg]['k3'] +=1\n",
    "            if sk>=3 and sk<4:\n",
    "                dataout2[lg]['k4'] +=1\n",
    "            if sk>=4 and sk<5:\n",
    "                dataout2[lg]['k5'] +=1\n",
    "            if sk>=5 and sk<6:\n",
    "                dataout2[lg]['k6'] +=1\n",
    "            if sk>=6 and sk<7:\n",
    "                dataout2[lg]['k7'] +=1\n",
    "            if sk>=7 and sk<8:\n",
    "                dataout2[lg]['k8'] +=1\n",
    "            if sk>=8 and sk<9:\n",
    "                dataout2[lg]['k9'] +=1\n",
    "            if sk>=9:\n",
    "                dataout2[lg]['k10'] +=1\n",
    "                \n",
    "        if sig!= '-1' and sigtypenumberdict[sig] in possibleset:\n",
    "            if sk == 0:\n",
    "                dataout2[lg]['p0'] +=1\n",
    "            if sk>0 and sk<1:\n",
    "                dataout2[lg]['p1'] +=1\n",
    "            if sk>=1 and sk<2:\n",
    "                dataout2[lg]['p2'] +=1\n",
    "            if sk>=2 and sk<3:\n",
    "                dataout2[lg]['p3'] +=1\n",
    "            if sk>=3 and sk<4:\n",
    "                dataout2[lg]['p4'] +=1\n",
    "            if sk>=4 and sk<5:\n",
    "                dataout2[lg]['p5'] +=1\n",
    "            if sk>=5 and sk<6:\n",
    "                dataout2[lg]['p6'] +=1\n",
    "            if sk>=6 and sk<7:\n",
    "                dataout2[lg]['p7'] +=1\n",
    "            if sk>=7 and sk<8:\n",
    "                dataout2[lg]['p8'] +=1\n",
    "            if sk>=8 and sk<9:\n",
    "                dataout2[lg]['p9'] +=1\n",
    "            if sk>=9:\n",
    "                dataout2[lg]['p10'] +=1\n",
    "                \n",
    "        if sig!='-1' and sigtypenumberdict[sig] in mightdropset:\n",
    "            if sk <=1:\n",
    "                dataout2[lg]['m0'] +=1\n",
    "            if sk>1:\n",
    "                dataout2[lg]['m1'] +=1\n",
    "            \n",
    "        if sig!='-1' and sigtypenumberdict[sig] in dropset:\n",
    "            if sk <= 1:\n",
    "                dataout2[lg]['d0'] +=1\n",
    "            if sk>1:\n",
    "                dataout2[lg]['d1'] +=1\n",
    "#save data for later usage\n",
    "fopen2 = open(\"/home/zwen/Desktop/stat/m2.csv\",'w')\n",
    "writer = csv.DictWriter(fopen2, fieldnames=fieldnames1)\n",
    "writer.writeheader()\n",
    "for lg in lognameset:\n",
    "    writer.writerow(dataout2[lg])\n",
    "fopen2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data3\n",
    "#random choose from lognamet and lognamef set individually 10,000 samples without replacement'''\n",
    "import numpy as np\n",
    "lognametlist = list(lognamet)\n",
    "lognameflist = list(lognamef)\n",
    "'''randomly choose 10,000 from both lognametlist and lognameflist'''\n",
    "rd_lognametlist = np.random.choice(lognametlist,60000,replace=False)\n",
    "rd_lognameflist = np.random.choice(lognameflist,60000,replace=False)\n",
    "rd_logname = []\n",
    "rd_logname.extend(rd_lognametlist)\n",
    "rd_logname.extend(rd_lognameflist)\n",
    "'''\n",
    "generate a new file with 10000 true label and 10000 false label\n",
    "'''\n",
    "fieldnames3=['logname','aroundinc','label']\n",
    "for i in range(11):\n",
    "    fieldnames3.append('k'+str(i))\n",
    "for i in range(0,11,2):\n",
    "    fieldnames3.append('p'+str(i))\n",
    "for i in range(0,11,5):\n",
    "    fieldnames3.append('m'+str(i))\n",
    "for i in range(0,11,10):\n",
    "    fieldnames3.append('d'+str(i))\n",
    "    \n",
    "dataout3 = defaultdict(dict)\n",
    "for lg in rd_logname:\n",
    "    \n",
    "    for i in range(11):\n",
    "        dataout3[lg]['k'+str(i)] = 0\n",
    "    for i in range(0,11,2):\n",
    "        dataout3[lg]['p'+str(i)] = 0\n",
    "    for i in range(0,11,5):\n",
    "        dataout3[lg]['m'+str(i)] = 0\n",
    "    for i in range(0,11,10):\n",
    "        dataout3[lg]['d'+str(i)] = 0\n",
    "    dataout3['label'] = 0\n",
    "\n",
    "g = lambda x : 1 if x=='t' else 0    \n",
    "for lg in rd_logname:\n",
    "    dataout3[lg]['aroundinc']= g(aroundinc[lg])\n",
    "    dataout3[lg]['logname'] = lg\n",
    "    if isinc[lg] == 't':\n",
    "        dataout3[lg]['label'] = 1\n",
    "    else:\n",
    "        dataout3[lg]['label'] = 0\n",
    "        \n",
    "    for sig in sigfreqlogIntscore[lg].keys():\n",
    "        sk = sigfreqlogIntscore[lg][sig]\n",
    "        if sig!= '-1' and sigtypenumberdict[sig] in keepset:\n",
    "            if sk == 0:\n",
    "                dataout3[lg]['k0'] +=1\n",
    "            if sk>0 and sk<0.1:\n",
    "                dataout3[lg]['k1'] +=1\n",
    "            if sk>=0.1 and sk<0.2:\n",
    "                dataout3[lg]['k2'] +=1\n",
    "            if sk>=0.2 and sk<0.3:\n",
    "                dataout3[lg]['k3'] +=1\n",
    "            if sk>=0.3 and sk<0.4:\n",
    "                dataout3[lg]['k4'] +=1\n",
    "            if sk>=0.4 and sk<0.5:\n",
    "                dataout3[lg]['k5'] +=1\n",
    "            if sk>=0.5 and sk<0.6:\n",
    "                dataout3[lg]['k6'] +=1\n",
    "            if sk>=0.6 and sk<0.7:\n",
    "                dataout3[lg]['k7'] +=1\n",
    "            if sk>=0.7 and sk<0.8:\n",
    "                dataout3[lg]['k8'] +=1\n",
    "            if sk>=0.8 and sk<0.9:\n",
    "                dataout3[lg]['k9'] +=1\n",
    "            if sk>=0.9 and sk<=1:\n",
    "                dataout3[lg]['k10'] +=1\n",
    "                \n",
    "        if sig!= '-1' and sigtypenumberdict[sig] in possibleset:\n",
    "            if sk == 0:\n",
    "                dataout3[lg]['p0'] +=1\n",
    "            if sk>0 and sk<0.2:\n",
    "                dataout3[lg]['p2'] +=1\n",
    "            if sk>=0.2 and sk<0.4:\n",
    "                dataout3[lg]['p4'] +=1\n",
    "            if sk>=0.4 and sk<0.6:\n",
    "                dataout3[lg]['p6'] +=1\n",
    "            if sk>=0.6 and sk<0.8:\n",
    "                dataout3[lg]['p8'] +=1\n",
    "            if sk>=0.8 and sk<=1:\n",
    "                dataout3[lg]['p10'] +=1\n",
    "                \n",
    "        if sig!='-1' and sigtypenumberdict[sig] in mightdropset:\n",
    "            if sk == 0:\n",
    "                dataout3[lg]['m0'] +=1\n",
    "            if sk>0 and sk<0.5:\n",
    "                dataout3[lg]['m5'] +=1\n",
    "            if sk>=0.5 and sk<=1:\n",
    "                dataout3[lg]['m10'] +=1\n",
    "        if sig!='-1' and sigtypenumberdict[sig] in dropset:\n",
    "            if sk < 0.8:\n",
    "                dataout3[lg]['d0'] +=1\n",
    "            if sk>=0.8 and sk<=1:\n",
    "                dataout3[lg]['d10'] +=1\n",
    "fopen3 = open(\"/home/zwen/Desktop/stat/rd_m36.csv\",'w')\n",
    "writer = csv.DictWriter(fopen3, fieldnames=fieldnames3)\n",
    "writer.writeheader()\n",
    "for lg in rd_logname:\n",
    "    writer.writerow(dataout3[lg])\n",
    "fopen3.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''data4'''\n",
    "'''random choose from lognamet and lognamef set individually 10,000 samples without replacement'''\n",
    "import numpy as np\n",
    "lognametlist = list(lognamet)\n",
    "lognameflist = list(lognamef)\n",
    "'''randomly choose 10,000 from both lognametlist and lognameflist'''\n",
    "rd_lognametlist = np.random.choice(lognametlist,10000,replace=False)\n",
    "rd_lognameflist = np.random.choice(lognameflist,10000,replace=False)\n",
    "rd_logname = []\n",
    "rd_logname.extend(rd_lognametlist)\n",
    "rd_logname.extend(rd_lognameflist)\n",
    "\n",
    "\n",
    "fieldnames4=['logname','aroundinc','label']\n",
    "for i in range(11):\n",
    "    fieldnames4.append('k'+str(i))\n",
    "for i in range(11):\n",
    "    fieldnames4.append('p'+str(i))\n",
    "for i in range(2):\n",
    "    fieldnames4.append('m'+str(i))\n",
    "for i in range(2):\n",
    "    fieldnames4.append('d'+str(i))\n",
    "\n",
    "dataout4 = defaultdict(dict)\n",
    "for lg in rd_logname:\n",
    "    for i in range(11):\n",
    "        dataout4[lg]['k'+str(i)] = 0\n",
    "    for i in range(11):\n",
    "        dataout4[lg]['p'+str(i)] = 0\n",
    "    for i in range(2):\n",
    "        dataout4[lg]['m'+str(i)] = 0\n",
    "    for i in range(2):\n",
    "        dataout4[lg]['d'+str(i)] = 0\n",
    "    dataout4[lg]['label'] = 0\n",
    "    \n",
    "  \n",
    "   \n",
    "\n",
    "g = lambda x : 1 if x=='t' else 0    \n",
    "for lg in rd_logname:\n",
    "    dataout4[lg]['aroundinc']= g(aroundinc[lg])\n",
    "    dataout4[lg]['logname'] = lg\n",
    "    if isinc[lg] == 't':\n",
    "        dataout4[lg]['label'] = 1\n",
    "    else:\n",
    "        dataout4[lg]['label'] = 0\n",
    "        \n",
    "    for sig in sigfreqlogIntscore[lg].keys():\n",
    "        sk = sigfreqlogIntscoretimesfreq[lg][sig]\n",
    "        if sig!= '-1' and sigtypenumberdict[sig] in keepset:\n",
    "            if sk == 0:\n",
    "                dataout4[lg]['k0'] +=1\n",
    "            if sk>0 and sk<1:\n",
    "                dataout4[lg]['k1'] +=1\n",
    "            if sk>=1 and sk<2:\n",
    "                dataout4[lg]['k2'] +=1\n",
    "            if sk>=2 and sk<3:\n",
    "                dataout4[lg]['k3'] +=1\n",
    "            if sk>=3 and sk<4:\n",
    "                dataout4[lg]['k4'] +=1\n",
    "            if sk>=4 and sk<5:\n",
    "                dataout4[lg]['k5'] +=1\n",
    "            if sk>=5 and sk<6:\n",
    "                dataout4[lg]['k6'] +=1\n",
    "            if sk>=6 and sk<7:\n",
    "                dataout4[lg]['k7'] +=1\n",
    "            if sk>=7 and sk<8:\n",
    "                dataout4[lg]['k8'] +=1\n",
    "            if sk>=8 and sk<9:\n",
    "                dataout4[lg]['k9'] +=1\n",
    "            if sk>=9:\n",
    "                dataout4[lg]['k10'] +=1\n",
    "                \n",
    "        if sig!= '-1' and sigtypenumberdict[sig] in possibleset:\n",
    "            if sk == 0:\n",
    "                dataout4[lg]['p0'] +=1\n",
    "            if sk>0 and sk<1:\n",
    "                dataout4[lg]['p1'] +=1\n",
    "            if sk>=1 and sk<2:\n",
    "                dataout4[lg]['p2'] +=1\n",
    "            if sk>=2 and sk<3:\n",
    "                dataout4[lg]['p3'] +=1\n",
    "            if sk>=3 and sk<4:\n",
    "                dataout4[lg]['p4'] +=1\n",
    "            if sk>=4 and sk<5:\n",
    "                dataout4[lg]['p5'] +=1\n",
    "            if sk>=5 and sk<6:\n",
    "                dataout4[lg]['p6'] +=1\n",
    "            if sk>=6 and sk<7:\n",
    "                dataout4[lg]['p7'] +=1\n",
    "            if sk>=7 and sk<8:\n",
    "                dataout4[lg]['p8'] +=1\n",
    "            if sk>=8 and sk<9:\n",
    "                dataout4[lg]['p9'] +=1\n",
    "            if sk>=9:\n",
    "                dataout4[lg]['p10'] +=1\n",
    "                \n",
    "        if sig!='-1' and sigtypenumberdict[sig] in mightdropset:\n",
    "            if sk <=1:\n",
    "                dataout4[lg]['m0'] +=1\n",
    "            if sk>1:\n",
    "                dataout4[lg]['m1'] +=1\n",
    "            \n",
    "        if sig!='-1' and sigtypenumberdict[sig] in dropset:\n",
    "            if sk <= 1:\n",
    "                dataout4[lg]['d0'] +=1\n",
    "            if sk>1:\n",
    "                dataout4[lg]['d1'] +=1\n",
    "fopen4 = open(\"/home/zwen/Desktop/stat/rd_m4.csv\",'w')\n",
    "writer = csv.DictWriter(fopen4, fieldnames=fieldnames4)\n",
    "writer.writeheader()\n",
    "for lg in rd_logname:\n",
    "    writer.writerow(dataout4[lg])\n",
    "fopen4.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''generate data five WHERER ENTRY IS THE LOG(FREQUENCY) +1'''\n",
    "'''add field titles'''\n",
    "'''data5 took too long to run'''\n",
    "fieldnames5=['logname','aroundinc','label']\n",
    "lsig = []\n",
    "for sig in sigset:\n",
    "    fieldnames5.append('x'+sig+'x')\n",
    "    lsig.append('x'+sig+'x')\n",
    "\n",
    "dataout5 = {}\n",
    "\n",
    "fopen5 = open(\"/home/zwen/Desktop/stat/m5.csv\",'w')\n",
    "writer = csv.DictWriter(fopen5, fieldnames=fieldnames5)\n",
    "writer.writeheader()\n",
    "\n",
    "g = lambda x : 1 if x=='t' else 0    \n",
    "for lg in lognameset:\n",
    "    dataout5['aroundinc']= g(aroundinc[lg])\n",
    "    dataout5['logname'] = lg\n",
    "    if isinc[lg] == 't':\n",
    "        dataout5['label'] = 1\n",
    "    else:\n",
    "        dataout5['label'] = 0\n",
    "    for sig in lsig:   \n",
    "        if sig in sigfreqlog[lg].keys():\n",
    "            sk = sigfreqlog[lg][sig]\n",
    "            if(sk>0):\n",
    "                '''+1 FOR THE CASE WHERE SK = 1'''\n",
    "                dataout5['x'+sig+'x'] = np.log(sk)+1\n",
    "        else:\n",
    "            dataout5['x'+sig+'x'] = 0\n",
    "    writer.writerow(dataout5)\n",
    "\n",
    "fopen5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''add field titles'''\n",
    "'''data6  choose signatures with frequence cross data set >=100\n",
    "   signature value = log(fre)+1\n",
    "'''\n",
    "\n",
    "'''random choose from lognamet and lognamef set individually 6,000 samples without replacement'''\n",
    "import numpy as np\n",
    "lognametlist = list(lognamet)\n",
    "lognameflist = list(lognamef)\n",
    "'''randomly choose 10,000 from both lognametlist and lognameflist'''\n",
    "rd_lognametlist = np.random.choice(lognametlist,6000,replace=False)\n",
    "rd_lognameflist = np.random.choice(lognameflist,6000,replace=False)\n",
    "rd_logname = []\n",
    "rd_logname.extend(rd_lognametlist)\n",
    "rd_logname.extend(rd_lognameflist)\n",
    "\n",
    "fieldnames6=['logname','aroundinc','label']\n",
    "lsig=[]\n",
    "for sig in lsiggreatertime:\n",
    "    fieldnames6.append('x'+sig+'x')\n",
    "    lsig.append(sig);\n",
    "  \n",
    "        \n",
    "\n",
    "dataout6 = {}\n",
    "\n",
    "fopen6 = open(\"/Users/grandwor/Desktop/research/m8.csv\",'w')\n",
    "writer = csv.DictWriter(fopen6, fieldnames=fieldnames6)\n",
    "writer.writeheader()\n",
    "\n",
    "g = lambda x : 1 if x=='t' else 0  \n",
    "count = 0  \n",
    "for lg in rd_logname:\n",
    "    count +=1\n",
    "    dataout6['aroundinc']= g(aroundinc[lg])\n",
    "    dataout6['logname'] = lg\n",
    "    if isinc[lg] == 't':\n",
    "        dataout6['label'] = 1\n",
    "    else:\n",
    "        dataout6['label'] = 0\n",
    "    for sig in lsig:   \n",
    "        if sig in sigfreqlog[lg].keys():\n",
    "            sk = sigfreqlog[lg][sig]\n",
    "            if(sk>0):\n",
    "                '''+1 FOR THE CASE WHERE SK = 1'''\n",
    "                dataout6['x'+sig+'x'] = np.log(sk)+1\n",
    "        else:\n",
    "            dataout6['x'+sig+'x'] = 0\n",
    "    print(\"row count is \",count)\n",
    "    writer.writerow(dataout6)\n",
    "\n",
    "fopen6.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''add field titles'''\n",
    "'''data7  choose signatures with frequence cross data set >=50\n",
    "   signature value using tf-idf = freq*(log(N/n+1)) smooth\n",
    "'''\n",
    "\n",
    "'''random choose from lognamet and lognamef set individually 10,000 samples without replacement'''\n",
    "import numpy as np\n",
    "lognametlist = list(lognamet)\n",
    "lognameflist = list(lognamef)\n",
    "'''number of lines of data\n",
    "   number of signatures across all lines of data sigdatacount\n",
    "'''\n",
    "Nline = len(lognameset)\n",
    "\n",
    "'''randomly choose 10,000 from both lognametlist and lognameflist'''\n",
    "rd_lognametlist = np.random.choice(lognametlist,6000,replace=False)\n",
    "rd_lognameflist = np.random.choice(lognameflist,6000,replace=False)\n",
    "rd_logname = []\n",
    "rd_logname.extend(rd_lognametlist)\n",
    "rd_logname.extend(rd_lognameflist)\n",
    "\n",
    "fieldnames7=['logname','aroundinc','label']\n",
    "lxsigx = []\n",
    "lsig=[]\n",
    "    \n",
    "for sig in lsiggreatertime:\n",
    "    fieldnames7.append('x'+sig+'x')\n",
    "    lsig.append(sig)\n",
    "  \n",
    "\n",
    "dataout7 = {}\n",
    "\n",
    "fopen7 = open(\"/Users/grandwor/Desktop/research_stat/m14.csv\",'w')\n",
    "writer = csv.DictWriter(fopen7, fieldnames=fieldnames7)\n",
    "writer.writeheader()\n",
    "\n",
    "g = lambda x : 1 if x=='t' else 0  \n",
    "count = 0  \n",
    "for lg in rd_logname:\n",
    "    count +=1\n",
    "    dataout7['aroundinc']= g(aroundinc[lg])\n",
    "    dataout7['logname'] = lg\n",
    "    if isinc[lg] == 't':\n",
    "        dataout7['label'] = 1\n",
    "    else:\n",
    "        dataout7['label'] = 0\n",
    "    for sig in lsig:   \n",
    "        if sig in sigfreqlog[lg].keys():\n",
    "            sk = sigfreqlog[lg][sig]\n",
    "            if(sk>0):\n",
    "                '''+1 FOR THE CASE WHERE SK = 1'''\n",
    "                dataout7['x'+sig+'x'] = (np.log(sk)+1)*np.log(Nline/sigdatacount[sig]+1)\n",
    "        else:\n",
    "            dataout7['x'+sig+'x'] = 0\n",
    "    print(\"row count is \",count)\n",
    "    writer.writerow(dataout7)\n",
    "\n",
    "fopen7.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''''\n",
    "'''add field titles'''\n",
    "'''data_m12  choose signatures with frequence cross data set >=50\n",
    "   signature value = log(fre)+1\n",
    "'''\n",
    "\n",
    "'''random choose from lognamet and lognamef set individually 10,000 samples without replacement'''\n",
    "import numpy as np\n",
    "lognametlist = list(lognamet)\n",
    "lognameflist = list(lognamef)\n",
    "'''randomly choose 10,000 from both lognametlist and lognameflist'''\n",
    "rd_lognametlist = np.random.choice(lognametlist,10000,replace=False)\n",
    "rd_lognameflist = np.random.choice(lognameflist,10000,replace=False)\n",
    "rd_logname = []\n",
    "rd_logname.extend(rd_lognametlist)\n",
    "rd_logname.extend(rd_lognameflist)\n",
    "\n",
    "fieldnames6=['logname','aroundinc','label']\n",
    "\n",
    "lsig = []\n",
    "for sig in lsiggreatertime:\n",
    "    fieldnames6.append('x'+sig+'x')   \n",
    "    lsig.append(sig)\n",
    "\n",
    "        \n",
    "\n",
    "dataout8 = {}\n",
    "\n",
    "fopen8 = open(\"/Users/grandwor/Desktop/research_stat/m12.csv\",'w')\n",
    "writer = csv.DictWriter(fopen8, fieldnames=fieldnames6)\n",
    "writer.writeheader()\n",
    "\n",
    "g = lambda x : 1 if x=='t' else 0  \n",
    "count = 0  \n",
    "for lg in rd_logname:\n",
    "    count +=1\n",
    "    dataout8['aroundinc']= g(aroundinc[lg])\n",
    "    dataout8['logname'] = lg\n",
    "    if isinc[lg] == 't':\n",
    "        dataout8['label'] = 1\n",
    "    else:\n",
    "        dataout8['label'] = 0\n",
    "    for sig in lsig:   \n",
    "        if sig in sigfreqlog[lg].keys():\n",
    "            sk = sigfreqlog[lg][sig]\n",
    "            if(sk>0):\n",
    "                '''+1 FOR THE CASE WHERE SK = 1'''\n",
    "                dataout8['x'+sig+'x'] = np.log(sk)+1\n",
    "                print(\"sig \",sig,\" lgfre \", dataout8[sig])\n",
    "        else:\n",
    "            dataout8['x'+sig+'x'] = 0\n",
    "    print(\"row count is \",count)\n",
    "    writer.writerow(dataout8)\n",
    "fopen8.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out different models for the previous generated data sets 1-8\n",
    "from simple to complex\n",
    "starts from logistic\n",
    "then SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''logistic regression on data set 1'''\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "data1 = pd.read_csv(\"/home/zwen/Desktop/stat/m1.csv\")\n",
    "fieldnames1=['logname','aroundinc','label']\n",
    "for i in range(11):\n",
    "    fieldnames1.append('k'+str(i))\n",
    "for i in range(0,11,2):\n",
    "    fieldnames1.append('p'+str(i))\n",
    "for i in range(0,11,5):\n",
    "    fieldnames1.append('m'+str(i))\n",
    "for i in range(0,11,10):\n",
    "    fieldnames1.append('d'+str(i))\n",
    "data1.columns=fieldnames1\n",
    "data1.head()\n",
    "'''get the train and test data'''\n",
    "X= data1.iloc[:,3:].values\n",
    "y =data1.iloc[:,2].values\n",
    "\n",
    "X_train,X_test, y_train, y_test =train_test_split(X,y,test_size=.3, random_state=25)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "'''test'''\n",
    "y_pred=logreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix=confusion_matrix(y_test,y_pred)\n",
    "confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''train with SVM on data set 1'''\n",
    "from sklearn.svm import SVC\n",
    "svcrbf=SVC()\n",
    "svclinear=SVC(kernel='linear')\n",
    "model=svclinear.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix=confusion_matrix(y_test,y_pred)\n",
    "confusion_matrix\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''logistic regression and SVM on data2'''\n",
    "\n",
    "fieldnames2=['logname','aroundinc','label']\n",
    "for i in range(11):\n",
    "    fieldnames2.append('k'+str(i))\n",
    "for i in range(11):\n",
    "    fieldnames2.append('p'+str(i))\n",
    "for i in range(2):\n",
    "    fieldnames2.append('m'+str(i))\n",
    "for i in range(2):\n",
    "    fieldnames2.append('d'+str(i))\n",
    "data2 = pd.read_csv(\"/home/zwen/Desktop/stat/m2.csv\")\n",
    "data2.columns = fieldnames2\n",
    "X= data2.iloc[:,3:].values\n",
    "y =data2.iloc[:,2].values\n",
    "\n",
    "X_train,X_test, y_train, y_test =train_test_split(X,y,test_size=.3, random_state=25)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "'''test'''\n",
    "y_pred=logreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix=confusion_matrix(y_test,y_pred)\n",
    "confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "data1 = pd.read_csv(\"/home/zwen/Desktop/stat/m1.csv\")\n",
    "fieldnames1=['logname','aroundinc','label']\n",
    "for i in range(11):\n",
    "    fieldnames.append('k'+str(i))\n",
    "for i in range(0,11,2):\n",
    "    fieldnames.append('p'+str(i))\n",
    "for i in range(0,11,5):\n",
    "    fieldnames.append('m'+str(i))\n",
    "for i in range(0,11,10):\n",
    "    fieldnames.append('d'+str(i))\n",
    "data1.columns=fieldnames1\n",
    "data1.head()\n",
    "'''get the train and test data'''\n",
    "X= data1.iloc[:,3:].values\n",
    "y =data1.iloc[:,2].values\n",
    "\n",
    "X_train,X_test, y_train, y_test =train_test_split(X,y,test_size=.3, random_state=25)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "'''test'''\n",
    "y_pred=logreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix=confusion_matrix(y_test,y_pred)\n",
    "confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "'''train with SVM'''\n",
    "from sklearn.svm import SVC\n",
    "model=SVC().fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix=confusion_matrix(y_test,y_pred)\n",
    "confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''logistic regression and SVM classifier on rd_m3'''\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "data3 = pd.read_csv(\"/home/zwen/Desktop/stat/rd_m36.csv\")\n",
    "fieldnames3=['logname','aroundinc','label']\n",
    "for i in range(11):\n",
    "    fieldnames3.append('k'+str(i))\n",
    "for i in range(0,11,2):\n",
    "    fieldnames3.append('p'+str(i))\n",
    "for i in range(0,11,5):\n",
    "    fieldnames3.append('m'+str(i))\n",
    "for i in range(0,11,10):\n",
    "    fieldnames3.append('d'+str(i))\n",
    "data3.columns=fieldnames3\n",
    "data3.head()\n",
    "'''get the train and test data'''\n",
    "X= data3.iloc[:,3:].values\n",
    "y =data3.iloc[:,2].values\n",
    "\n",
    "X_train,X_test, y_train, y_test =train_test_split(X,y,test_size=.3, random_state=25)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "'''test'''\n",
    "y_pred=logreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix=confusion_matrix(y_test,y_pred)\n",
    "confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "'''train with SVM'''\n",
    "from sklearn.svm import SVC\n",
    "svclinear=SVC(kernel='linear')\n",
    "model=SVC().fit(X_train,y_train)\n",
    "modellinear=svclinear.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix=confusion_matrix(y_test,y_pred)\n",
    "confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''analysis on data set rd_m4'''\n",
    "\n",
    "'''logistic regression and SVC on rd_m4'''\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "data4 = pd.read_csv(\"/home/zwen/Desktop/stat/rd_m4.csv\")\n",
    "fieldnames4=['logname','aroundinc','label']\n",
    "for i in range(11):\n",
    "    fieldnames4.append('k'+str(i))\n",
    "for i in range(11):\n",
    "    fieldnames4.append('p'+str(i))\n",
    "for i in range(2):\n",
    "    fieldnames4.append('m'+str(i))\n",
    "for i in range(2):\n",
    "    fieldnames4.append('d'+str(i))\n",
    "data4.columns=fieldnames4\n",
    "data4.head()\n",
    "'''get the train and test data'''\n",
    "X= data4.iloc[:,3:].values\n",
    "y =data4.iloc[:,2].values\n",
    "\n",
    "X_train,X_test, y_train, y_test =train_test_split(X,y,test_size=.3, random_state=25)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "'''test'''\n",
    "y_pred=logreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix=confusion_matrix(y_test,y_pred)\n",
    "confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "'''train with SVM'''\n",
    "from sklearn.svm import SVC\n",
    "svclinear=SVC(kernel='linear')\n",
    "model=SVC().fit(X_train,y_train)\n",
    "modellinear=svclinear.fit(X_train,y_train)\n",
    "y_pred = modellinear.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix=confusion_matrix(y_test,y_pred)\n",
    "confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''logistic and svm on data 8\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "data8 = pd.read_csv(\"/Users/grandwor/Desktop/research/m8.csv\")\n",
    "\n",
    "data8.columns=fieldnames6\n",
    "data8.head()\n",
    "'''get the train and test data'''\n",
    "X= data8.iloc[:,3:].values\n",
    "y =data8.iloc[:,2].values\n",
    "\n",
    "X_train,X_test, y_train, y_test =train_test_split(X,y,test_size=.3, random_state=25)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "'''test'''\n",
    "y_pred=logreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix=confusion_matrix(y_test,y_pred)\n",
    "confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "'''train with SVM'''\n",
    "from sklearn.svm import SVC\n",
    "svcrbf=SVC()\n",
    "svclinear=SVC(kernel='linear')\n",
    "model=svclinear.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix=confusion_matrix(y_test,y_pred)\n",
    "confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''logistic and SVM on data 9\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "data9 = pd.read_csv(\"/Users/grandwor/Desktop/research/m9.csv\")\n",
    "\n",
    "data9.columns=fieldnames7\n",
    "data9.head()\n",
    "'''get the train and test data'''\n",
    "X= data9.iloc[:,3:].values\n",
    "y =data9.iloc[:,2].values\n",
    "\n",
    "X_train,X_test, y_train, y_test =train_test_split(X,y,test_size=.3, random_state=25)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "'''test'''\n",
    "y_pred=logreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix=confusion_matrix(y_test,y_pred)\n",
    "confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "'''train with SVM'''\n",
    "from sklearn.svm import SVC\n",
    "svcrbf=SVC()\n",
    "svclinear=SVC(kernel='linear')\n",
    "model=svclinear.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix=confusion_matrix(y_test,y_pred)\n",
    "confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''logistic and SVM on data 10\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "data10 = pd.read_csv(\"/Users/grandwor/Desktop/research_stat/m13.csv\")\n",
    "\n",
    "data10.columns=fieldnames6\n",
    "data10.head()\n",
    "'''get the train and test data'''\n",
    "X= data10.iloc[:,3:].values\n",
    "y =data10.iloc[:,2].values\n",
    "\n",
    "X_train,X_test, y_train, y_test =train_test_split(X,y,test_size=.3, random_state=25)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "'''test'''\n",
    "y_pred=logreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix=confusion_matrix(y_test,y_pred)\n",
    "confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "'''train with SVM'''\n",
    "from sklearn.svm import SVC\n",
    "svcrbf=SVC()\n",
    "svclinear=SVC(kernel='rbf')\n",
    "model=svclinear.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix=confusion_matrix(y_test,y_pred)\n",
    "confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''logistic and SVM on data 11\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "data11 = pd.read_csv(\"/Users/grandwor/Desktop/research_stat/m14.csv\")\n",
    "\n",
    "data11.columns=fieldnames7\n",
    "data11.head()\n",
    "'''get the train and test data'''\n",
    "X= data11.iloc[:,3:].values\n",
    "y =data11.iloc[:,2].values\n",
    "\n",
    "X_train,X_test, y_train, y_test =train_test_split(X,y,test_size=.3, random_state=25)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "'''test'''\n",
    "y_pred=logreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix=confusion_matrix(y_test,y_pred)\n",
    "confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "'''train with SVM'''\n",
    "from sklearn.svm import SVC\n",
    "svcrbf=SVC()\n",
    "svclinear=SVC(kernel='rbf')\n",
    "model=svclinear.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix=confusion_matrix(y_test,y_pred)\n",
    "confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''generate the Bayesian structures'''\n",
    "'''neighbordic'''\n",
    "'''local importance_ chance in the data set the signatures in logs labelled one'''\n",
    "localimpcount={}\n",
    "localnonimpcount={}\n",
    "for sig in lsiggreatertime:\n",
    "    localimpcount[sig]=0\n",
    "    localnonimpcount[sig] = 0\n",
    "    for lg in logsignature[sig]:\n",
    "        if lg in lognamet:\n",
    "            localimpcount[sig] +=1\n",
    "        if lg in lognamef:\n",
    "            localnonimpcount[sig]+=1\n",
    "localimp = {}\n",
    "for sig in lsiggreatertime:\n",
    "    k = len(logsignature[sig])\n",
    "    if k >0:\n",
    "        localimp[sig] = localimpcount[sig]/k\n",
    "    else:\n",
    "        localimp[sig] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''generate the Bayesian structures'''\n",
    "'''neighbordic'''\n",
    "signb = {}\n",
    "count1 = 0\n",
    "for sig in lsiggreatertime:\n",
    "    count1 +=1\n",
    "    print(\"Number of sig \", count1)\n",
    "    signb[sig]={}\n",
    "    for lg in logsignature[sig]:\n",
    "        if sig in siglog[lg]:\n",
    "            for s in siglog[lg]:\n",
    "                if s in lsiggreatertime:\n",
    "                    '''check whether the co-neighbor frequency alread caluculate in signb[s][sig]'''\n",
    "                    '''elif: first time to calculate this neighborhood information'''\n",
    "                    if s in signb.keys() and s != sig:\n",
    "                        signb[sig][s] = signb[s][sig]\n",
    "               \n",
    "                    elif s in signb[sig].keys():\n",
    "                        signb[sig][s] +=1\n",
    "                    else:\n",
    "                        signb[sig][s] = 1\n",
    "                        \n",
    "'''change count to the probability'''\n",
    "signbprob = {}\n",
    "sum = {}\n",
    "for sig in signb.keys():\n",
    "    sum[sig] = 0\n",
    "    for s in signb[sig].keys():\n",
    "        sum[sig] +=signb[sig][s]\n",
    "for sig in signb.keys():\n",
    "    signbprob[sig] ={}\n",
    "    for s in signb[sig].keys():\n",
    "        signbprob[sig][s] = float(signb[sig][s]/signb[sig][sig])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''collective learning, try to get stable  probabability from its surrounding one-step-away neighbors\n",
    "weight is calculated in the signbprob, and the normalized proximity score is used as the initial node importance\n",
    "also use restart value, assign 0.2 for restart and right now I don't know how to normalize\n",
    "'''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigimp1 = {}\n",
    "'''initiate the sigimp[sig] to be the normalized proximity score'''\n",
    "for sig in sigset:\n",
    "    if sig != \"-1\":\n",
    "        sigimp1[sig] = normproxintdict[sig]\n",
    "    else:\n",
    "        sigimp1[sig] = 0.1\n",
    "sigimp = {}\n",
    "'''initiate the sigimp[sig] to be the normalized proximity score'''\n",
    "for sig in sigset:\n",
    "    if sig != \"-1\":\n",
    "        sigimp[sig] = normproxintdict[sig]\n",
    "    else:\n",
    "        sigimp[sig] = 0.1\n",
    "limit = 0.1;\n",
    "'''normalization factor'''\n",
    "nf = 1\n",
    "count = 0\n",
    "while(limit>0.1):\n",
    "    limit = 0 \n",
    "    count +=1\n",
    "    for sig in lsiggreatertime: \n",
    "        sum = 0\n",
    "        \n",
    "        for s in signbprob[sig].keys():\n",
    "            '''update the importance by the neighbor d importance weighted by the neighbor probability'''\n",
    "            sum += nf*sigimp[s]*signbprob[sig][s]\n",
    "        '''print(\"sig is \", sig,\"sigimp\", sigimp[sig])'''  \n",
    "        if sum>1:\n",
    "            sum = 1\n",
    "        limit +=abs(sum - sigimp[sig])\n",
    "        print(\"limit is \",limit) \n",
    "        sigimp[sig] = sum\n",
    "    print(\"iterate count\", count)\n",
    "    print(\"limit is \",limit) \n",
    "    \n",
    "    \n",
    "def write_report(r1, r2, filename):\n",
    "        input_file=open(filename, \"a\")\n",
    "        for k in r1.keys():\n",
    "            line = '{}, {} {}'.format(k,r1[k],r2[k])\n",
    "            print>>input_file, line\n",
    "filename1 = \"/home/zwen/Desktop/stat/sigimpcomparison.txt\"\n",
    "write_report(sigimp,sigimp1,filename1)\n",
    "for sig in sigimp1.keys():\n",
    "    print(\"sig is\", sig, \" difference is imp1 - imp is \",sigimp1[sig] - sigimp[sig])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
